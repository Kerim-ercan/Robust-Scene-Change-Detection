##############################################
# CONFIGURATION FILE FOR FINE TUNING PROCESS #
##############################################

###################
# model selection #
###################

model:
    checkpoints: "/home/divit/projects/models/dinov2.2CrossAttn.Diff-CMU.pth"  # Update this path
    target-shp-row: 504
    target-shp-col: 504      # Adjust based on your image size

#############
# optimizer #
#############

# ---------
optimizer:
    epochs: 20                   # You might want to increase epochs for custom dataset
    warmup-epoch: 2
    learn-rate: 0.00002         # Slightly lower learning rate for fine-tuning
    loss-weight: True
    lr-scheduler: "cosine"       # Use cosine scheduler for better convergence
    grad-scaler: True            # enable mixed precision

###########
# dataset #
###########

# -------
dataset:
    batch-size: 4                # Adjust based on your GPU memory
    num-workers: 4
    dataset: "CUSTOM"            # Use your custom dataset
    hflip-prob: 0.5
    figsize: 224                 # Optional: specify if different from default

##############
# evaluation #
##############

# ----------
evaluation:
    # name: epoch frequency
    trainset: 5                  # Evaluate on train set every 5 epochs

#########
# wandb #
#########

# ------
wandb:
    project: "custom-change-detection"  # Your project name
    name: "custom-dataset-finetune"     # Experiment name
    output-path: ""                     # Will be auto-generated if empty
    save-checkpoint-freq: 5             # Save checkpoint every 5 epochs

########################
# develope and testing #
########################

# --------------------
environment:
    dry: False                   # Set to True for quick testing
    seed: 123
    verbose: True
    device: "cuda"